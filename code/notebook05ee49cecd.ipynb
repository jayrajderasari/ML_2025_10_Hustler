{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf368bc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-14T16:21:04.714704Z",
     "iopub.status.busy": "2025-04-14T16:21:04.714424Z",
     "iopub.status.idle": "2025-04-14T16:21:06.847147Z",
     "shell.execute_reply": "2025-04-14T16:21:06.846228Z"
    },
    "papermill": {
     "duration": 2.137856,
     "end_time": "2025-04-14T16:21:06.848597",
     "exception": false,
     "start_time": "2025-04-14T16:21:04.710741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/temp-data/wavelet_features_reduced.xlsx\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a5bec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:21:06.853715Z",
     "iopub.status.busy": "2025-04-14T16:21:06.853408Z",
     "iopub.status.idle": "2025-04-14T16:21:12.016843Z",
     "shell.execute_reply": "2025-04-14T16:21:12.016084Z"
    },
    "papermill": {
     "duration": 5.16777,
     "end_time": "2025-04-14T16:21:12.018740",
     "exception": false,
     "start_time": "2025-04-14T16:21:06.850970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (7.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aca3f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:21:12.025822Z",
     "iopub.status.busy": "2025-04-14T16:21:12.025204Z",
     "iopub.status.idle": "2025-04-14T16:22:07.842884Z",
     "shell.execute_reply": "2025-04-14T16:22:07.842001Z"
    },
    "papermill": {
     "duration": 55.823732,
     "end_time": "2025-04-14T16:22:07.846027",
     "exception": false,
     "start_time": "2025-04-14T16:21:12.022295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "# Function to monitor memory usage\n",
    "def memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory = process.memory_info().rss / 1024 / 1024  # in MB\n",
    "    return f\"Memory usage: {memory:.2f} MB\"\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(\"/kaggle/input/temp-data/wavelet_features_reduced.xlsx\")\n",
    "\n",
    "# Rename columns (adjust if necessary)\n",
    "df.columns = [\"Person\", \"Image\", \"Mean\", \"Variance\", \"Energy\"]\n",
    "\n",
    "# Features and label\n",
    "X = df[[\"Mean\", \"Variance\", \"Energy\"]]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"Person\"])  # Encode person IDs\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"dataset loading done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1412dec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:22:07.850999Z",
     "iopub.status.busy": "2025-04-14T16:22:07.850667Z",
     "iopub.status.idle": "2025-04-14T16:22:07.872122Z",
     "shell.execute_reply": "2025-04-14T16:22:07.871486Z"
    },
    "papermill": {
     "duration": 0.025214,
     "end_time": "2025-04-14T16:22:07.873271",
     "exception": false,
     "start_time": "2025-04-14T16:22:07.848057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401907</th>\n",
       "      <td>0.160383</td>\n",
       "      <td>0.034157</td>\n",
       "      <td>1119.724041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306750</th>\n",
       "      <td>0.162920</td>\n",
       "      <td>0.017914</td>\n",
       "      <td>1018.263479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569722</th>\n",
       "      <td>0.206594</td>\n",
       "      <td>0.023451</td>\n",
       "      <td>1593.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571340</th>\n",
       "      <td>0.165008</td>\n",
       "      <td>0.048454</td>\n",
       "      <td>1284.971977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226391</th>\n",
       "      <td>0.219424</td>\n",
       "      <td>0.046007</td>\n",
       "      <td>1957.080068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>0.212076</td>\n",
       "      <td>0.053807</td>\n",
       "      <td>1916.190724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365838</th>\n",
       "      <td>0.228050</td>\n",
       "      <td>0.039063</td>\n",
       "      <td>2021.086069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>0.217715</td>\n",
       "      <td>0.022241</td>\n",
       "      <td>1735.072464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671155</th>\n",
       "      <td>0.172725</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>1169.313511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>0.269108</td>\n",
       "      <td>0.047346</td>\n",
       "      <td>2766.204374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548629 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mean  Variance       Energy\n",
       "401907  0.160383  0.034157  1119.724041\n",
       "306750  0.162920  0.017914  1018.263479\n",
       "569722  0.206594  0.023451  1593.000285\n",
       "571340  0.165008  0.048454  1284.971977\n",
       "226391  0.219424  0.046007  1957.080068\n",
       "...          ...       ...          ...\n",
       "259178  0.212076  0.053807  1916.190724\n",
       "365838  0.228050  0.039063  2021.086069\n",
       "131932  0.217715  0.022241  1735.072464\n",
       "671155  0.172725  0.023107  1169.313511\n",
       "121958  0.269108  0.047346  2766.204374\n",
       "\n",
       "[548629 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f445b2db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:22:07.879100Z",
     "iopub.status.busy": "2025-04-14T16:22:07.878864Z",
     "iopub.status.idle": "2025-04-14T16:22:09.931811Z",
     "shell.execute_reply": "2025-04-14T16:22:09.930901Z"
    },
    "papermill": {
     "duration": 2.057483,
     "end_time": "2025-04-14T16:22:09.933136",
     "exception": false,
     "start_time": "2025-04-14T16:22:07.875653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading assumed complete.\n",
      "Scaling done. 682.09 MB\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:01<00:00, 38.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done. 684.32 MB\n",
      "Prediction done. 695.95 MB\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.20      0.13     14003\n",
      "           1       0.11      0.01      0.02     13875\n",
      "           2       0.10      0.15      0.12     13814\n",
      "           3       0.10      0.21      0.14     14107\n",
      "           4       0.00      0.00      0.00     14034\n",
      "           5       0.00      0.00      0.00     14127\n",
      "           6       0.10      0.34      0.16     14011\n",
      "           7       0.10      0.04      0.06     13836\n",
      "           8       0.00      0.00      0.00     14058\n",
      "           9       0.10      0.05      0.07     14135\n",
      "\n",
      "    accuracy                           0.10    140000\n",
      "   macro avg       0.07      0.10      0.07    140000\n",
      "weighted avg       0.07      0.10      0.07    140000\n",
      "\n",
      "\n",
      "Total runtime: 1.87 seconds\n",
      "Final memory usage: 695.95 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Function to monitor memory usage\n",
    "def memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory = process.memory_info().rss / 1024 / 1024  # in MB\n",
    "    return memory\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load dataset (commented out since assumed loaded)\n",
    "# df = pd.read_excel(\"/kaggle/input/temp-data/wavelet_features_reduced.xlsx\")\n",
    "print(\"Dataset loading assumed complete.\")\n",
    "\n",
    "# Features and label (uncomment for actual data)\n",
    "# X = df[[\"Mean\", \"Variance\", \"Energy\"]]\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(df[\"Person\"])\n",
    "\n",
    "# Split data (uncomment for actual data)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Simulate data for runtime estimation (replace with your actual X_train, y_train)\n",
    "n_samples, n_features = 560000, 3\n",
    "X_train = np.random.rand(n_samples, n_features)  # Replace with actual data\n",
    "y_train = np.random.randint(0, 10, n_samples)    # Replace with actual labels\n",
    "X_test = np.random.rand(int(n_samples * 0.25), n_features)\n",
    "y_test = np.random.randint(0, 10, int(n_samples * 0.25))\n",
    "\n",
    "# Scale features (critical for SGDClassifier)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(f\"Scaling done. {memory_usage():.2f} MB\")\n",
    "\n",
    "# Initialize SGDClassifier with tuned parameters\n",
    "clf = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    max_iter=10000,  # More iterations for better convergence\n",
    "    tol=1e-3,\n",
    "    learning_rate='adaptive',  # Adjusts learning rate dynamically\n",
    "    eta0=0.01,  # Initial learning rate\n",
    "    alpha=0.0001,  # Regularization\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train with partial_fit for memory efficiency\n",
    "batch_size = 10000\n",
    "n_batches = len(X_train) // batch_size + 1\n",
    "\n",
    "print(\"Training started...\")\n",
    "for i in tqdm(range(n_batches)):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(X_train))\n",
    "    if start_idx < len(X_train):\n",
    "        clf.partial_fit(X_train[start_idx:end_idx], y_train[start_idx:end_idx], classes=np.unique(y_train))\n",
    "print(f\"Training done. {memory_usage():.2f} MB\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Prediction done. {memory_usage():.2f} MB\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(f\"\\nTotal runtime: {runtime:.2f} seconds\")\n",
    "print(f\"Final memory usage: {memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9834a423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:22:09.941935Z",
     "iopub.status.busy": "2025-04-14T16:22:09.941727Z",
     "iopub.status.idle": "2025-04-14T16:23:25.225635Z",
     "shell.execute_reply": "2025-04-14T16:23:25.224845Z"
    },
    "papermill": {
     "duration": 75.289089,
     "end_time": "2025-04-14T16:23:25.226899",
     "exception": false,
     "start_time": "2025-04-14T16:22:09.937810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading assumed complete.\n",
      "Training started...\n",
      "Training done. 914.56 MB\n",
      "Prediction done. 926.41 MB\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.07      0.08     14001\n",
      "           1       0.10      0.06      0.08     13892\n",
      "           2       0.10      0.13      0.11     13979\n",
      "           3       0.10      0.07      0.09     14002\n",
      "           4       0.10      0.10      0.10     13938\n",
      "           5       0.10      0.19      0.13     14097\n",
      "           6       0.10      0.19      0.13     14265\n",
      "           7       0.10      0.06      0.07     14052\n",
      "           8       0.10      0.06      0.08     13755\n",
      "           9       0.10      0.08      0.09     14019\n",
      "\n",
      "    accuracy                           0.10    140000\n",
      "   macro avg       0.10      0.10      0.10    140000\n",
      "weighted avg       0.10      0.10      0.10    140000\n",
      "\n",
      "\n",
      "Total runtime: 74.92 seconds\n",
      "Current memory usage: 22.66 MB\n",
      "Peak memory usage: 205.22 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import time\n",
    "import numpy as np\n",
    "import tracemalloc\n",
    "\n",
    "# Function to monitor memory usage\n",
    "def memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory = process.memory_info().rss / 1024 / 1024  # in MB\n",
    "    return memory\n",
    "\n",
    "# Start memory tracking\n",
    "tracemalloc.start()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load dataset (uncomment for actual data)\n",
    "# df = pd.read_excel(\"/kaggle/input/temp-data/wavelet_features_reduced.xlsx\")\n",
    "print(\"Dataset loading assumed complete.\")\n",
    "\n",
    "# Features and label (uncomment for actual data)\n",
    "# X = df[[\"Mean\", \"Variance\", \"Energy\"]]\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(df[\"Person\"])\n",
    "\n",
    "# Split data (uncomment for actual data)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Simulate data for runtime estimation (replace with your actual X_train, y_train)\n",
    "n_samples, n_features = 560000, 3\n",
    "X_train = np.random.rand(n_samples, n_features)  # Replace with actual data\n",
    "y_train = np.random.randint(0, 10, n_samples)    # Replace with actual labels\n",
    "X_test = np.random.rand(int(n_samples * 0.25), n_features)\n",
    "y_test = np.random.randint(0, 10, int(n_samples * 0.25))\n",
    "\n",
    "# Note: RandomForest doesn't require scaling, so we skip StandardScaler\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=150,  # More trees for better accuracy and longer runtime\n",
    "    max_depth=15,      # Deeper trees for better performance\n",
    "    min_samples_split=10,\n",
    "    n_jobs=-1,         # Use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Training started...\")\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Training done. {memory_usage():.2f} MB\")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Prediction done. {memory_usage():.2f} MB\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(f\"\\nTotal runtime: {runtime:.2f} seconds\")\n",
    "\n",
    "# Memory stats\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current memory usage: {current / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Peak memory usage: {peak / 1024 / 1024:.2f} MB\")\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cc664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T16:13:51.489889Z",
     "iopub.status.busy": "2025-04-14T16:13:51.489355Z",
     "iopub.status.idle": "2025-04-14T16:15:37.205000Z",
     "shell.execute_reply": "2025-04-14T16:15:37.203072Z",
     "shell.execute_reply.started": "2025-04-14T16:13:51.489865Z"
    },
    "papermill": {
     "duration": 0.002831,
     "end_time": "2025-04-14T16:23:25.232907",
     "exception": false,
     "start_time": "2025-04-14T16:23:25.230076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745223b",
   "metadata": {
    "papermill": {
     "duration": 0.0028,
     "end_time": "2025-04-14T16:23:25.238590",
     "exception": false,
     "start_time": "2025-04-14T16:23:25.235790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7143223,
     "sourceId": 11404220,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 147.195605,
   "end_time": "2025-04-14T16:23:25.858803",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-14T16:20:58.663198",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
